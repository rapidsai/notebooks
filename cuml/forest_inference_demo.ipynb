{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Inference Library\n",
    "The forest inference library is used to load saved forest models of xgboost, lightgbm or protobuf and perform inference on them. It can be used to perform both classification and regression. This notebook shows how to use the Forest Inference library with xgboost and lightgbm models.\n",
    "\n",
    "The model accepts both numpy arrays and cuDF dataframes. In order to convert your dataset to cudf format please read the cudf documentation on https://rapidsai.github.io/projects/cudf/en/latest/. \n",
    "\n",
    "For additional information on the forest inference library please refer to the documentation on https://rapidsai.github.io/projects/cuml/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/saljain/miniconda3/envs/fil-test/lib/python3.7/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n",
      "/home/nfs/saljain/miniconda3/envs/fil-test/lib/python3.7/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "from cuml import ForestInference\n",
    "from cuml.test.utils import array_equal\n",
    "from cuml.utils.import_utils import has_xgboost, has_lightgbm\n",
    "\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_xgboost():\n",
    "    import xgboost as xgb\n",
    "else:\n",
    "    raise(\"Please install xgboost using the conda package,\"\n",
    "          \" Use conda install -c conda-forge xgboost \"\n",
    "          \"command to install xgboost\")\n",
    "    \n",
    "if has_lightgbm():\n",
    "    import lightgbm as lgb\n",
    "else:\n",
    "    raise(\"Please install lightgbm using the conda package,\"\n",
    "          \" Use conda install -c conda-forge lightgbm \"\n",
    "          \"command to install lightgbm\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification and regression data\n",
    "def simulate_data(m, n, k=2, random_state=None, classification=True):\n",
    "    if classification:\n",
    "        features, labels = make_classification(n_samples=m,\n",
    "                                               n_features=n,\n",
    "                                               n_informative=int(n/5),\n",
    "                                               n_classes=k,\n",
    "                                               random_state=random_state)\n",
    "    else:\n",
    "        features, labels = make_regression(n_samples=m,\n",
    "                                           n_features=n,\n",
    "                                           n_informative=int(n/5),\n",
    "                                           n_targets=1,\n",
    "                                           random_state=random_state)\n",
    "    return np.c_[features].astype(np.float32), \\\n",
    "        np.c_[labels].astype(np.float32).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for additional information on the xgboost library please refer to the documentation on : \n",
    "#### https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that trains the xgboost model and performs prediction on it as well\n",
    "def train_xgboost_model(X_train, y_train,\n",
    "                        X_validation,\n",
    "                        y_validation,\n",
    "                        num_rounds, classification):\n",
    "    print(\"Training the xgboost model and saving it to be used for inference\")\n",
    "    # set the xgboost model parameters\n",
    "    xgboost_params={}\n",
    "    params = {'silent': 1}\n",
    "    if classification:\n",
    "        params['eval_metric'] = 'error'\n",
    "        params['objective'] = 'binary:logistic'\n",
    "    else:\n",
    "        params['eval_metric'] = 'error'\n",
    "        params['objective'] = 'reg:squarederror'\n",
    "        params['base_score'] = 0.0\n",
    "    params['max_depth'] = 25\n",
    "    params.update(xgboost_params)\n",
    "    model_path = \"xgb.model\"\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "    # save the trained xgboost model\n",
    "    bst.save_model(model_path)\n",
    "\n",
    "    print(\"Perform validation using the xgboost model and\"\n",
    "          \" save the predicted output to compare with the Forest Inference library\")\n",
    "    # predict the xgboost model\n",
    "    dvalidation = xgb.DMatrix(X_validation, label=y_validation)\n",
    "    xgb_preds = bst.predict(dvalidation)\n",
    "\n",
    "    # if the model is used for classification then convert\n",
    "    # the predicted values into class labels\n",
    "    if classification:\n",
    "        xgb_preds = np.around(xgb_preds)\n",
    "\n",
    "    return xgb_preds, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for additional information on the lightgbm library please refer to the documentation on : \n",
    "#### https://lightgbm.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that trains the lightgbm model and performs prediction on it as well\n",
    "def train_lightgbm_model(X_train, y_train,\n",
    "                        X_validation,\n",
    "                        num_round):\n",
    "    print(\"Training the lightgbm model and saving it to be used for inference\")\n",
    "    # convert the data into the lightgbm input format\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    # select the params for the lightgbm model\n",
    "    param = {'objective': 'binary',\n",
    "             'metric': 'binary_logloss'}\n",
    "\n",
    "    # train the lightgbm model\n",
    "    bst = lgb.train(param, train_data, num_round)\n",
    "    \n",
    "    # path where the model is saved\n",
    "    model_path = \"lgb.model\"\n",
    "    bst.save_model(model_path)\n",
    "\n",
    "    print(\"Perform validation using the lightgbm model and\"\n",
    "          \" save the predicted output to compare with the Forest Inference library\")\n",
    "    # perform prediction on the lightgbm model\n",
    "    gbm_preds = bst.predict(X_validation)\n",
    "\n",
    "    return gbm_preds, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for creating the dataset\n",
    "classification = False\n",
    "n_rows = 10000\n",
    "n_columns = 100\n",
    "n_categories = 2\n",
    "random_state = np.random.RandomState(43210)\n",
    "\n",
    "# select the model on which you want to perform\n",
    "# inference\n",
    "select_model = 'xgboost'\n",
    "\n",
    "# num of iterations for which the model is trained\n",
    "num_rounds = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data :  (8000, 100)\n",
      "Shape of the validation data :  (2000, 100)\n",
      "Training the xgboost model and saving it to be used for inference\n",
      "Perform validation using the xgboost model and save the predicted output to compare with the Forest Inference library\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "X, y = simulate_data(n_rows, n_columns, n_categories,\n",
    "                     random_state=random_state,\n",
    "                     classification=classification)\n",
    "n_rows, n_columns = X.shape\n",
    "train_size = 0.8\n",
    "\n",
    "#split the dataset into training and validation splits\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, train_size=train_size)\n",
    "print(\"Shape of the training data : \", np.shape(X_train))\n",
    "print(\"Shape of the validation data : \", np.shape(X_validation))\n",
    "if select_model == 'xgboost':\n",
    "    trained_model_preds, model_path = train_xgboost_model(X_train, y_train,\n",
    "                                                          X_validation,\n",
    "                                                          y_validation,\n",
    "                                                          num_rounds,\n",
    "                                                          classification)\n",
    "elif select_model == 'lightgbm':\n",
    "    trained_model_preds, model_path = train_lightgbm_model(X_train,\n",
    "                                                           y_train,\n",
    "                                                           X_validation,\n",
    "                                                           num_rounds)\n",
    "else:\n",
    "    raise(\" This model is not supported, please choose either\"\n",
    "          \" an xgboost model or lightgbm model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The load function of the ForestInference class accepts the following parameters:\n",
    "        filename : str\n",
    "           Path to saved model file in a treelite-compatible format\n",
    "           (See https://treelite.readthedocs.io/en/latest/treelite-api.html\n",
    "        output_class : bool\n",
    "           If true, return a 1 or 0 depending on whether the raw prediction\n",
    "           exceeds the threshold. If False, just return the raw prediction.\n",
    "        threshold : float\n",
    "           Cutoff value above which a prediction is set to 1.0\n",
    "           Only used if the model is classification and output_class is True\n",
    "        algo : string name of the algo from (from algo_t enum)\n",
    "             'NAIVE' - simple inference using shared memory\n",
    "             'TREE_REORG' - similar to naive but trees rearranged to be more\n",
    "                              coalescing-friendly\n",
    "             'BATCH_TREE_REORG' - similar to TREE_REORG but predicting\n",
    "                                    multiple rows per thread block\n",
    "        model_type : str\n",
    "            Format of saved treelite model to load.\n",
    "            Can be 'xgboost', 'lightgbm', or 'protobuf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved xgboost model to return the forest in the format used as an input by the forest inference library\n",
    "fm = ForestInference.load(filename=model_path,\n",
    "                          algo='BATCH_TREE_REORG',\n",
    "                          output_class=classification,\n",
    "                          threshold=0.50,\n",
    "                          model_type=select_model)\n",
    "# perform prediction on the model loaded from path\n",
    "fil_preds = fm.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the labels predicted with the selected models and the \n",
    "# labels predicted by the ForestInference library are similar or not\n",
    "array_equal(trained_model_preds, fil_preds, tol=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
